# ðŸš€ BERT-LSTM-Attention Classifier â€“ Testing Suite for Google Colab  
> End-to-end **inference & evaluation** script for the BERT + Bi-LSTM + Attention model trained on Google Colab.  
> Everything runs on **T4 GPU** and saves results back to **Google Drive**.

---

## ðŸ“‹ Table of Contents
1. [Overview](#overview)  
2. [Prerequisites](#prerequisites)
3. [Pipeline](#pipeline)  
4. [Outputs](#outputs)     


---

## 1. Overview <a id="overview"></a>
This notebook / script:
- Loads a pre-trained **BERT-LSTM-Attention** model from **Google Drive**.
- Tests it on any JSON/JSONL dataset (`{"text": "...", "label": "DESC|META"}`).
- Produces **accuracy, precision, recall, F1, AUC-ROC**, confusion matrix, ROC curve, probability distributions.
- Saves **all artefacts** (plots, JSON, reports) to Drive for reproducibility.

---

## 2. Prerequisites <a id="prerequisites"></a>
| What | How |
|------|-----|
| **Google Colab** | Runtime âžœ GPU (T4) enabled |
| **Google Drive** | Mounted at `/content/drive` |
| **Dependencies** | Installed automatically via `!pip install â€¦` |
| **Model artefacts** | You must have: <br>`best_model.pt` (or `final_model.pt`) <br>`config.json` <br>generated by the training notebook |

---

## 3. Pipeline <a id="pipeline"></a>
<img width="1404" height="50" alt="image" src="https://github.com/user-attachments/assets/f0f9de82-f29e-400f-b85f-20e60d797fd7" />

flowchart TD
    A[Raw Text] --> T[AutoTokenizer<br/>max_length=128]
    T --> B[BERT-base-uncased<br/>768-D embeddings]
    B --> L[Bi-LSTM<br/>256-D hidden Ã— 2 directions]
    L --> A2[Attention Layer<br/>weighted 256-D vector]
    A2 --> D[Dropout 0.3]
    D --> C[Linear(256 â†’ 1)]
    C --> S[Sigmoid â†’ Probability]
    S --> CL[Class<br/>0=No-Exp<br>1=Experience]
1. **Upload your model & data**  
   - `best_model.pt` â†’ `/content/drive/MyDrive/ELMo_Project/model_outputs/`
   - `config.json` â†’ same folder
   - Test data â†’ `/content/sample_data/technical_descriptions_dataset.json` (or change `TEST_DATA_PATH`)

2. **Run the cell below in a Colab notebook**  
   ```python
   # Mount Drive & install once
   from google.colab import drive
   drive.mount('/content/drive')
   !pip install transformers torch scikit-learn tqdm matplotlib seaborn -q

   # Copy-paste the full testing script (this file) into the next cell

  

---
## 4.Outputs<a id="outputs"></a>
<img width="715" height="483" alt="image" src="https://github.com/user-attachments/assets/2d0509ab-c2ae-4466-b79d-0553103329b7" />

<img width="864" height="459" alt="image" src="https://github.com/user-attachments/assets/32408e31-0c1b-4f5f-b5c8-b6ac88068931" />
